{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAxj9IkmKG7UJsI2CizK26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salvinj/New_Project/blob/master/Chatbot_Sal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U2t9uO9hp5M"
      },
      "source": [
        "**Importing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Vs6oGihTgm"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSG0IMGxh1rm"
      },
      "source": [
        "Importing and Reading the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ngazyQxhmtm",
        "outputId": "c7739869-ce53-4de7-cc9c-caeef91f9685"
      },
      "source": [
        "f=open('/content/Chatbot.txt','r',errors='ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower()#converts text to lower case\n",
        "nltk.download('punkt')#using the punkt tokenizer\n",
        "nltk.download('wordnet')#using the WordNet dictionary\n",
        "sent_tokens=nltk.sent_tokenize(raw_doc)# converts doc to list of sentences\n",
        "word_tokens=nltk.word_tokenize(raw_doc)#converts doc to list of words\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0aoexh1HOe"
      },
      "source": [
        "**Example of sentence token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ckZBMrL1FtP",
        "outputId": "e5daa042-2b4c-431b-b810-165eafce1db9"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nartificial intelligence  \\n connected to: roboticsphilosophymachine learning\\nfrom wikipedia, the free encyclopedia\\n\\npart of a series on\\nartificial intelligence\\nmajor goals\\nartificial general intelligenceplanningcomputer visiongeneral game playingknowledge reasoningmachine learningnatural language processingrobotics\\napproaches\\nsymbolicdeep learningbayesian networksevolutionary algorithms\\nphilosophy\\nethicsexistential riskturing testchinese roomcontrol problemfriendly ai\\nhistory\\ntimelineprogressai winter\\ntechnology\\napplicationsprojectsprogramming languages\\nglossary\\nglossary\\nvte\\npart of a series on\\nmachine learning\\nand\\ndata mining\\n\\nproblems\\nclassificationclusteringregressionanomaly detectionautomlassociation rulesreinforcement learningstructured predictionfeature engineeringfeature learningonline learningsemi-supervised learningunsupervised learninglearning to rankgrammar induction\\nsupervised learning\\n(classification  regression)\\ndecision treesensembles baggingboostingrandom forestk-nnlinear regressionnaive bayesartificial neural networkslogistic regressionperceptronrelevance vector machine (rvm)support vector machine (svm)\\nclustering\\nbirchcurehierarchicalk-meansexpectationmaximization (em)\\ndbscanopticsmean-shift\\ndimensionality reduction\\nfactor analysisccaicaldanmfpcapgdt-sne\\nstructured prediction\\ngraphical models bayes netconditional random fieldhidden markov\\nanomaly detection\\nk-nnlocal outlier factor\\nartificial neural network\\nautoencodercognitive computingdeep learningdeepdreammultilayer perceptronrnn lstmgruesnrestricted boltzmann machinegansomconvolutional neural network u-nettransformerspiking neural networkmemtransistorelectrochemical ram (ecram)\\nreinforcement learning\\nq-learningsarsatemporal difference (td)\\ntheory\\nbiasvariance tradeoffcomputational learning theoryempirical risk minimizationoccam learningpac learningstatistical learningvc theory\\nmachine-learning venues\\nneuripsicmlmljmlrarxiv:cs.lg\\nrelated articles\\nglossary of artificial intelligencelist of datasets for machine-learning researchoutline of machine learning\\nvte\\npart of a series on the\\nevolutionary algorithm\\nartificial developmentartificial lifecellular evolutionary algorithmcultural algorithmdifferential evolutioneffective fitnessevolutionary computationevolution strategygaussian adaptationevolutionary multimodal optimizationgrammatical evolutionparticle swarm optimizationmemetic algorithmnatural evolution strategyneuroevolutionpromoter based genetic algorithmspiral optimization algorithmself-modifying codepolymorphic code\\ngenetic algorithm\\nchromosomeclonal selection algorithmcrossovermutationgenetic memorygenetic fuzzy systemsselectionfly algorithm\\ngenetic programming\\ncartesian genetic programminglinear genetic programmingmulti expression programmingschemaeuriskoparity benchmark\\nvte\\nartificial intelligence (ai) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality.',\n",
              " 'the distinction between the former and the latter categories is often revealed by the acronym chosen.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MiTub4e1pRB"
      },
      "source": [
        "**Example of word tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7wmfuVL1F5Z",
        "outputId": "4152fb4f-5196-4f14-d075-e7358be62c00"
      },
      "source": [
        "word_tokens[:2]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artificial', 'intelligence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dT7jx6zi541"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp34msNNi6IM"
      },
      "source": [
        "lemmer  = nltk.stem.WordNetLemmatizer()\n",
        "#Wordnet is a sematically -oriented dictionary of English included in NLTK\n",
        "def LemTokens(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict=dict((ord(punct),None)for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILXfiv_ildWq"
      },
      "source": [
        "**Defining the Greeting Fucntion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa7pXbT5leFe"
      },
      "source": [
        "GREET_INPUTS=(\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",\"namaste\")\n",
        "GREET_RESPONSES=[\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\",\"namaste\"]\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in  GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)\n",
        "      "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W71EejrYo4NW"
      },
      "source": [
        "**Response Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIIVnr5zo4l2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1zsCh8bpRW0"
      },
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sent_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"I am sorry!I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response=robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf1WbDUmrtbZ"
      },
      "source": [
        "**Defining  conversation start/end protocols**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6wfxponrsDB",
        "outputId": "f612af60-81f0-49ca-c27c-f94d4e1c98ee"
      },
      "source": [
        "flag=True\n",
        "print(\"BOT:My name is Sal.Let's have a conversation!Also,if you want to exit any time,just type Bye!\")\n",
        "while (flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks'or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT:You are welcome..\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT:\"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print(\"BOT:\",end=\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT:Goodbye! Take care <3\")\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOT:My name is Sal.Let's have a conversation!Also,if you want to exit any time,just type Bye!\n",
            "namaste\n",
            "BOT:hey\n",
            "greetings\n",
            "BOT:hey\n",
            "hello\n",
            "BOT:hello\n",
            "artificial intelligence\n",
            "BOT:"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'strong' ai is usually labelled as artificial general intelligence (agi) while attempts to emulate 'natural' intelligence have been called artificial biological intelligence (abi).\n",
            "ai\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT:one proposal to deal with this is to ensure that the first generally intelligent ai is 'friendly ai' and will be able to control subsequently developed ais.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxYWOwoKpSU0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZahJMgfhnSx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}